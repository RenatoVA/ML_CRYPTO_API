{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['open', 'high', 'low', 'close', 'volume', 'close_percentage',\n",
    "                'volume_percentage', 'daily_change', 'up_down',\n",
    "                'mv_7', 'mv_14', 'mv_21', 'volat_7', 'volat_14', 'volat_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ons(df):\n",
    "  df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "  df['close_percentage'] = df.groupby('symbol')['close'].pct_change()\n",
    "  df['volume_percentage'] = df.groupby('symbol')['volume'].pct_change()\n",
    "\n",
    "  df['mv_7'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(window=7).mean())\n",
    "  df['mv_14'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(window=14).mean())\n",
    "  df['mv_21'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(window=21).mean())\n",
    "\n",
    "  df['volat_7'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(window=7).std())\n",
    "  df['volat_14'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(window=14).std())\n",
    "  df['volat_21'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(window=21).std())\n",
    "\n",
    "  df['daily_change'] = df['high'] - df['low']\n",
    "  df['up_down'] = df['close']/df['open']\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(df, features):\n",
    "    scaler = StandardScaler()\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_training(df, features, train_ratio=0.8):\n",
    "    df['target'] = df['close'].shift(-1)\n",
    "    df = df.dropna(subset=['target'])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    X = df[features]\n",
    "    y = df['target']\n",
    "\n",
    "    train_size = int(train_ratio * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, df['timestamp'].iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(X_train, y_train):\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        objective='reg:squarederror'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicts(model, X_test, y_test, scaler, features):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred_descaled = scaler.inverse_transform([[0] * (len(features) - 1) + [val] for val in y_pred])[:, -1]\n",
    "    y_test_descaled = scaler.inverse_transform([[0] * (len(features) - 1) + [val] for val in y_test])[:, -1]\n",
    "\n",
    "    return y_pred_descaled, y_test_descaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_precios(df, features):\n",
    "    scalers = {}\n",
    "    df_predictions = pd.DataFrame()\n",
    "    models = {}\n",
    "    tokens = df['symbol'].unique()\n",
    "    for token in tokens:\n",
    "        df_token = df[df['symbol'] == token].copy()\n",
    "\n",
    "        df_token, scaler = data_scaler(df_token, features)\n",
    "        scalers[token] = scaler\n",
    "\n",
    "        X_train, X_test, y_train, y_test, timestamps = before_training(df_token, features)\n",
    "\n",
    "        model = model_training(X_train, y_train)\n",
    "        models[token] = model\n",
    "        y_pred_descaled, y_test_descaled = predicts(model, X_test, y_test, scaler, features)\n",
    "\n",
    "        token_predictions = pd.DataFrame({\n",
    "            'symbol': token,\n",
    "            'timestamp': timestamps,\n",
    "            'y_test': y_test_descaled,\n",
    "            'y_pred': y_pred_descaled\n",
    "        })\n",
    "        df_predictions = pd.concat([df_predictions, token_predictions], ignore_index=True)\n",
    "\n",
    "    return df_predictions, models, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_X_future_prices(days_to_predict, models, df, scalers, features):\n",
    "  tokens = df['symbol'].unique()\n",
    "  future_prices_per_token = {}\n",
    "\n",
    "  for token in tokens:\n",
    "      model=models[token]\n",
    "      df_token = df[df['symbol'] == token].copy()\n",
    "      X_last = df_token[features].iloc[-1].copy().values.reshape(1, -1)\n",
    "      ultima_fecha = df_token['timestamp'].max()\n",
    "\n",
    "      predicciones_futuras = []\n",
    "\n",
    "      for i in range(days_to_predict):\n",
    "          y_next = model.predict(X_last)\n",
    "\n",
    "          y_next_descaled = scalers[token].inverse_transform([[0] * (len(features) - 1) + [val] for val in y_next])[:, -1]\n",
    "          fecha_prediccion = ultima_fecha + pd.Timedelta(days=i + 1)\n",
    "          predicciones_futuras.append({'fecha': fecha_prediccion, 'prediccion': y_next_descaled[0]})\n",
    "\n",
    "          X_last = np.concatenate([X_last[:, 1:], [[y_next[0]]]], axis=1)\n",
    "\n",
    "      future_prices_per_token[token] = pd.DataFrame(predicciones_futuras)\n",
    "\n",
    "  for token, predicciones in future_prices_per_token.items():\n",
    "      print(f\"Predicciones para los próximos {days_to_predict} días para {token}:\")\n",
    "      print(predicciones)\n",
    "\n",
    "  return future_prices_per_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaming = pd.read_csv('../data/processed/gaming.csv')\n",
    "meme = pd.read_csv('../data/processed/meme.csv')\n",
    "ai = pd.read_csv('../data/processed/ai.csv')\n",
    "rwa = pd.read_csv('../data/processed/rwa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaming = add_ons(gaming)\n",
    "meme = add_ons(meme)\n",
    "ai = add_ons(ai)\n",
    "rwa = add_ons(rwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([gaming, meme, ai, rwa], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_gaming, model_gaming, scalers_gaming = predecir_precios(gaming, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_ai, model_ai, scalers_ai = predecir_precios(ai, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_meme, model_meme, scalers_meme = predecir_precios(meme, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_rwa, model_rwa, scalers_rwa = predecir_precios(rwa, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_folder = '../data/models/'\n",
    "scaler_folder = '../data/scalers/'\n",
    "\n",
    "\n",
    "\n",
    "for token, model in model_gaming.items():\n",
    "    model_filename = f\"{model_folder}{token}_model.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "for token, model in model_ai.items():\n",
    "    model_filename = f\"{model_folder}{token}_model.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "for token, model in model_meme.items():\n",
    "    model_filename = f\"{model_folder}{token}_model.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "for token, model in model_rwa.items():\n",
    "    model_filename = f\"{model_folder}{token}_model.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    \n",
    "for token, scaler in scalers_gaming.items():\n",
    "    scaler_filename = f\"{scaler_folder}{token}_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "for token, scaler in scalers_ai.items():\n",
    "    scaler_filename = f\"{scaler_folder}{token}_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "for token, scaler in scalers_meme.items():\n",
    "    scaler_filename = f\"{scaler_folder}{token}_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "for token, scaler in scalers_rwa.items():\n",
    "    scaler_filename = f\"{scaler_folder}{token}_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
